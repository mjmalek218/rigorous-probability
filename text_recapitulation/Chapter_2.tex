\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=0.5in]{geometry}

\begin{document}

\title{Chapter Two: Probability Triples}

\maketitle

\medskip 


\begin{abstract}

This chapter covers probability triples, the foundation of the theory, and how to construct them. In particular, a great deal of effort is spent setting the foundations for what to do in the case of uncountable sample spaces, as the basic theory does not generalize easily when taking the obvious choices for a sigma-algebra. 

\end{abstract}

\bigskip

\section{Basic Definition} 

\noindent \textbf{Definition.} A \emph{probability triple} is compromised of three components:

\begin{enumerate}

\item A \emph{sample space} $\Omega$ which is the set of all values we are considering.

\item A \emph{$\sigma$-algebra} $\mathcal{F}$ which is a sigma algebra over the sample space (recall this is a collection fo sets which is closed under complement and countable union, as well as containing $\emptyset$). 

\item A probability measure $\mathbb{P}$ which is a mapping $\mathbb{P}:\mathcal{F} \to [0,1]$ that is also countably additive and such that $P(\emptyset) = 0$, $P(\Omega) = 1$. 

\end{enumerate}

Note that the fact that $\mathcal{F}$ is closed under countable unions and complements implies it is closed under countable intersections as well. By deMorgan's Laws for set operations, we have that $(A^c \cup B^c)^c = A \cap B$, and so we can apply this to countably infinite operations via a reasonable limiting process. 

Also note that there are several basic familiar results we can derive regarding this definition of a probability triple. 

\begin{enumerate}

\item Complements: Consider $P(A \cup A^c) = 1 = P(A) + P(A^c)$. Thus the familiar 1 - rule for probability complements. 

\item Setminus: Consider arbitrary $A$ and $B$. Then \[P(A) = P((A\setminus (B\cap A)) \cup (B \cap A)) = P[A\setminus (B\cap A)] + P[B \cap A] \] \[ = P[A\setminus B] + P[B \cap A] \Rightarrow  P[A\setminus B] = P(A) - P(B \cap A) \]

\item Inclusion/Exclusion: We want to derive a result for $P(A \cup B)$. Note that \[P(A \cup B) = P[ (A\setminus B) \cup (B \cap A) \cup (B \setminus A) ] = P(A) - 2P(A \cap B) + P(A \cap B) + P(B)  \]

\item Obviously countable subadditivity follows as well. 
  
\end{enumerate}

With these rules much of probability theory can be derived. Indeed, these are more or less the start point for Jaynes in his book, with a few exceptions. 


\section{Constructing Probability Triples}

Now the question becomes how to apply these rules to actual scenarios. In the case of countable $\Omega$, this is simple: just provide a probability measure which adds to 1 on the entire set, and the sigma algebra will be the set of all subsets of $\Omega$. Take for example the Poisson(5) distribution, and let $A$ be an arbitrary subset. This yields the following measure:

\[P(A) = \Sigma_{k \in A} e^{-5} 5^k / k! \] Also we can easily recover the uniform distribution over any finite set from this method as well. 

The challenge now comes from how to create a valid and meaningful probability triple over an uncountable set. We examine $[0,1]$ for simplicity. First one might consider the set \[\mathcal{J} = \text{all intervals contained in [0,1]}. \] Unfortunately this cannot be considered a sigma algebra, as the union of two intervals is not an interval. However, it is clearly a semi-algebra: an essential tool for building the theory. 

\medskip

\noindent \textbf{Definition.} A \emph{semi-algebra} is a collection of subsets of a sample space $\Omega$ such that  it is closed under finite intersection, contains $\Omega$ and $\emptyset$, and the complement of every element is a disjoint union of elements in the set.

\medskip

This is more or less a generalization of the example provided: in all likelihood, the historical origin of this notion comes from considering all intervals of a set. We prove this connection in the exercises. 

However, it is not easy to generalize to a probability triple from this semi-algebra collection: a few obvious extensions fall markedly short, including considering the set of all finite unions of the semi-algebra and all countable unions (exercise 2.4.7). Thus...the next section. 

\section{The Extension Theorem}

The extension theorem is the primary method we have for creating probability triples on uncountable sample spaces. It essentially begins with a semi-algebra and a function on that algebra (a proto-measure, so to speak) satisfying certain key conditions. 

\medskip

\noindent \textbf{Theorem.} Consider any sample space $\Omega$, a semi-algebra $\mathcal{J}$ on that sample space and a function $P: \; \mathcal{J} \to [0,1]$ such that 

\begin{enumerate}

\item Finite Additivity: for any finite collection of disjoint sets in $\mathcal{J}$ we have that \[P(\cup A_i) = \sum P(A_i) \] where the sum and unions are taken to be over all $i$. 

\item $P(\Omega) = 1$ and $P(\emptyset) = 0$.

\item Countable monotonicity property: if we have a collection of sets $\{A_i\}$ of cardinality $\aleph_0$, and set which is a subset of a union of these sets, then the sum of $P(A_i)$ over all the $i$ 

\end{enumerate} 

\noindent Then there exists a $\sigma$-algebra $\mathcal{M} \supseteq \mathcal{J}$ and a probability measure $P^*$ on $\mathcal{M}$ which agress with the function $P$ for all $A \in \mathcal{J}$. 

\medskip

The proof of this theorem is complex, and involves many different results which we go onto prove here. 

\medskip

\noindent \textbf{Definition. 2.3.4.} The \emph{outer measure} extension of the function $P$ given in the extension theorem is denoted as $P^*$ and defined as the infimum of the sum over all subcollections of sets in $\mathcal{J}$, where the union of these sub-sollection is a superset of the input set. Basically we go from above and get smaller.  

\medskip

Now that we have this definition for the outermeasure, we can move forward. Essentially, it is the measure we will define for the extension, but in order for it to be appropriate we will need to restrict it to a certain $\sigma$-algebra. In the mean time, we can prove a necessary property which holds for any set in $\mathcal{J}$. 

\medskip

\noindent \textbf{Lemma. 2.3.5.} $P^*(A) = P(A)$ for all $A \in \mathcal{J}$. 

\noindent \textbf{Proof.} Consider any arbitrary $A \in \mathcal{J}$. Then $P^*(A) \leq P(A)$ because we can just set the sum to be the single set $A$, so we know the infimum is at most as large as $P(A)$. The reverse holds by countable monotonicity. 

\hfill $\Box$

\medskip

Next we prove another necessary result which holds regardless of the the input sets, and without the restriction: countable subadditivity. 

\medskip

\noindent \textbf{Lemma. 2.3.6.} $P^*$ is countably subadditive: \[P^*(\cup_{n=1}^\infty B_n) \leq \sum_{n=1}^\infty P^*(B_n) \;\;\; \text{for any} \;\;\; B_1, B_2, \cdots \subseteq \Omega \]

\noindent \textbf{Proof.} This proof essentially involves a simple bounding argument. Let $\{A_{n,i}\}_{i = 0}^\infty$ be a collection of subsets of $\Omega$ where \[ P^*(B_n) + \frac{\epsilon}{2^n} > \sum P(A_i)\] where $\epsilon > 0$. Such a collection must exist by the definition of the outer measure as an infimum. Summing over all the $n$ we have the following relation: \[\sum_{n=1}^\infty P^*(B_n) + \epsilon >  \sum_{\forall n} \sum_{\forall i} P(A_{i,n}).\] But since the union of the overall collection $\{A_{n,i}\}_{i = 0}^\infty$ contains the union of the $B_n$ what we really have is \[\sum_{n=1}^\infty P^*(B_n) + \epsilon >  \sum_{\forall n} \sum_{\forall i} P(A_{i,n}) \geq P^*(\cup_{n=1}^\infty B_n) \] which holds for any arbitrary epsilon and therefore completes the proof. 

\hfill $\Box$

\medskip

\noindent \textbf{Definition. 2.3.7} The Lebesgue $\sigma$-algebra is \[\mathcal{M} = \{A \subseteq \Omega \mid P^*(A \cap E) + P^*(A^c \cap E) = P(E)\}.\]

\medskip

\noindent \textbf{Lemma 2.3.9.} If $A_1, A_2, \cdots \in \mathcal{M}$ are disjoint, then $P^*(\cup_n A_n) = \sum_n P^*(A_n)$. 

\noindent \textbf{Proof.} Letting $A_1 \cup A_2 = E$, by definition we have \[P(A_1 \cap E) + P(A_2 \cap E) = P(A_1 \cup A_2) \;\; \Rightarrow \;\; P(A_1) + P(A_2) = P(A_1 \cup A_2)\] where the implication follows since $A_1$ and $A_2$ are disjoint. So this holds for \emph{any} finite collection, by induction. 

Now we generalize to an arbitrary countable collection. Consider any $m \in \mathbb{N}$. Then by all our previous work/definitions: 

\[\sum_{\forall n} P^*(A_n) \geq P^*(\cup_{\forall n} A_n) \geq P^*( \cup_{n \leq m} A_n) = \sum_{n \leq m} P^*(A_n).\] This holds for any $m$, so taking the limit we have equality at infinity. 

\hfill $\Box$  

\medskip

The above result demonstrates why defining $\mathcal{M}$ was important in the first place: because countable additivity over disjoint sets might not have held otherwise. Now that we have finished proving results for $P^*$ on $\mathcal{M}$, we need only demonstrate that $\mathcal{M}$ has the algebraic closure properties we desire (interestingly enough, the properties we demonstrate are true here are set-analogous to those in Jaynes for a Boolean algebra which covers all possible logic functions: but I digress).

\bigskip

\noindent \textbf{Lemma 2.3.10} $\mathcal{M}$ is an algebra (only proved for finite intersection). 

\noindent \textbf{Proof.} First note closure under complements is trivial, and that $\mathcal{M}$ obviously contains $\Omega$. Consider $A$, $B \in \mathcal{M}$. By countable subadditivity of the outer-measure we have that \[P^*( (A \cap B) \cap E) + P^*( (A \cap B)^c \cap E) \geq P^*(E).\] We just need to prove the reverse direction and we are finished. \[P^*((A \cap B) \cap E) + P^*( (A \cap B)^c \cap E) = P^*(A \cap B \cap E) + P^*( (A^c \cap B \cap E) \cup (A \cap B^c \cap E) \cup (A^c \cap B^c \cap E)).\] Now we can apply subadditivity of the measure and the fact that $A,B \in \mathcal{M}$. \[P^*(A \cap B \cap E) + P^*( (A^c \cap B \cap E) \cup (A \cap B^c \cap E) \cup (A^c \cap B^c \cap E))\] \[ \leq P^*(A \cap B \cap E) + P^*( (A^c \cap B \cap E)) + P^*(A \cap B^c \cap E)) + P^*(A^c \cap B^c \cap E)) \] \[= P^*(B \cap E) + P(B^c \cap E)) = P(E).\]

\hfill $\Box$

\medskip

Now the following lemma is really just mathematical machinery we need for our subsequent proof that $\mathcal{M}$ is a $\sigma$-algebra. But it makes intuitive sense, because of the way the outer-measure is defined: we want the outer-measure to be closed under taking the intersection with some arbitrary set. That is to say, if the $E$ were removed here it would follow clearly, but because the $E$ is there it is because of the way it is defined. 

\medskip

\noindent \textbf{Lemma 2.3.11} Let $A_1, A_2, \cdots \in \mathcal{M}$ be disjoint. For each $m \in \mathbb{N}$, let $B_m = \cup_{n \leq m} A_n$. Then for all $m \in \mathbb{N}$, and for all $E \subseteq \Omega$, \[P^*(E \cap B_m) = \sum_{n \leq m} P^*(E \cap A_n). \]

\medskip

\noindent \textbf{Proof.} We prove this inductively. The base case, $m = 1$, holds trivially. Now we extrapolate and assume the proposition holds true for $m = k$. 

\[P^*(E \cap B_{m+1}) = P^*(E \cap B_{m+1} \cap B_m) + P^*(E \cap B_{m+1} \cap B_m^c) \] where this equality follows since  $B_m \in \mathcal{M}$.  But we are essentially done now because by assumption we have \[P^*(E \cap B_{m+1} \cap B_m^c) =  P^*(E \cap A_m)\] and the other portion is just the original sum (by the inductive hypothesis). 

\hfill $\Box$

\medskip

Now using this result we can prove that elements of $\mathcal{M}$ are closed under countable union. 

\medskip

\noindent \textbf{Lemma 2.3.13} Let $A_1$, $A_2$, $\cdots \in \mathcal{M}$ be disjoint. Then $\cup_n A_n \in \mathcal{M}$.  

\medskip

\noindent \textbf{Proof.} First let $B_m = \cup_{n \leq m} A_n$. Then by preceding lemma we have \[P^*(E) = P^*(B_m \cap E) + P^*(B_m^c \cap E) \geq \sum_{n \leq m} P^*(A_n \cap E) + P^*((\cup_n A_n)^c \cap E)\] where the inequality follows since $B_m^c \supset (\cup_n A_n)^c$. This inequality folows for any $m$, and so we can extend it to \[\bigg( \sum_{n \leq m} P^*(A_n \cap E) \bigg) + P^*((\cup_n A_n)^c \cap E) \geq \bigg( \sum_{n} P^*(A_n \cap E) \bigg) + P^*((\cup_n A_n)^c \cap E)\] which finally yields the desired result: \[ P^*(E) \geq P^*(\cup_n A_n \cap E) + P^*((\cup_n A_n)^c \cap E).\]

\hfill $\Box$

\medskip

\noindent \textbf{Lemma 2.3.14} $\mathcal{M}$ is a $\sigma$-algebra. 

\medskip

\noindent \textbf{Proof.} By the previous lemma, we can demonstrate $\mathcal{M}$ is countably subadditive. The rest follows from the fact that $\mathcal{M}$ is an algebra (lemma 2.3.10).

To demonstrate that Lemma 2.3.13 will hold even for non-disjoint sets, consider any aribtrary sequence of sets. Then let every $n^{th}$ set in a new sequence of sets created from this given sequence be whatever new elements the original set in the sequence contributes. These sets are disjoint but the union is the same. 

\hfill $\Box$

\medskip

\noindent \textbf{Lemma 2.3.15} $\mathcal{J} \subseteq \mathcal{M}$. 


% FINISH THIS PROOF FINISH THIS PROOF

\medskip

\noindent \textbf{Extension Theorem} *The original theorem stated at the beginning of this subsection*


\section{Constructing the Uniform[0,1] distribution}

We can now apply the previous result 


\end{document}